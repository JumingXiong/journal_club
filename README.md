# Journal-Club
Time: Friday morning 10:00 - 10:30 AM, FGH 313

- [Journal-Club](#Journal-Club)
	- [Paper-Reading-Group](#paper-reading-group)
  
## Paper-Reading-Group

Agenda

|Date|Speaker|Paper|Remark|
|---|:---:|---|---|
|2024.03.29|Cathy Cui  <br>  （Efficient Model）  |[《Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts》](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2312.00968.pdf)
|2024.03.29|Cathy Cui  <br>  （Efficient Model）  |[《EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything》](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2312.00863.pdf)
|2024.03.22|Juming Xiong    <br>  （Generative Model）  |[《Endora: Video Generation Models as Endoscopy Simulators》](https://arxiv.org/pdf/2403.11050.pdf)
|2024.03.22|Juming Xiong    <br>  （Image Segmentation）|[《OMG-Seg: Is One Model Good Enough For All Segmentation》(CVPR 2024)](https://arxiv.org/pdf/2401.10229.pdf)
|2024.03.22|Juming Xiong    <br>  （Image registration）|[《Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration》(CVPR 2024)](https://arxiv.org/pdf/2402.18933.pdf)
|2024.03.15|Yucheng Tang    <br>  （Autoregressive Models）  |[《Taming Transformers for High-Resolution Image Synthesis》(CVPR 2021)](https://arxiv.org/pdf/2012.09841.pdf)
|2024.03.15|Yucheng Tang    <br>  （Autoregressive Models）  |[《Sequential Modeling Enables Scalable Learning for Large Vision Models》](https://arxiv.org/pdf/2312.00785.pdf)
|2024.03.15|Yucheng Tang    <br>  （Autoregressive Models）  |[《VILA: On Pre-training for Visual Language Models》(CVPR 2024)](https://arxiv.org/pdf/2312.07533.pdf)
|2024.03.01|Junlin Guo    <br>  （Visual Language model + Dataset denoising）  |[《Filtering, distillation, and hard negatives for vision-language pre-training》(CVPR 2023)](https://arxiv.org/pdf/2301.02280.pdf)
|2024.03.01|Junlin Guo    <br>  （Foundation model + Weakly supervised learning）  |[《Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation》(CVPR 2023)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Foundation_Model_Drives_Weakly_Incremental_Learning_for_Semantic_Segmentation_CVPR_2023_paper.pdf)
|2024.03.01|Junlin Guo    <br>  （Self-supervised Pre-training）  |[《Geometric Visual Similarity Learning in 3D Medical Image Self-supervised Pre-training》(CVPR 2023)](https://openaccess.thecvf.com/content/CVPR2023/html/He_Geometric_Visual_Similarity_Learning_in_3D_Medical_Image_Self-Supervised_Pre-Training_CVPR_2023_paper.html)
|2024.02.23|Tianyuan Yao    <br>  （Vision 'language' Model）  |[《Images Speak in Images: A Generalist Painter for In-Context Visual Learning](https://arxiv.org/pdf/2212.02499.pdf)
|2024.02.23|Tianyuan Yao    <br>  （Machine unlearning）  |[《UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models](https://arxiv.org/pdf/2402.11846.pdf)
|2024.02.16|Marilyn Lionts     <br>  （Unlearnable Datasets）  |[《UNLEARNABLE EXAMPLES: MAKING PERSONAL DATA UNEXPLOITABLE》(ICLR2021）](https://arxiv.org/pdf/2101.04898.pdf)
|2024.02.16|Marilyn Lionts     <br>  （Unlearnable Datasets）  |[《CUDA: Convolution-based Unlearnable Datasets》(CVPR 2023）](https://openaccess.thecvf.com/content/CVPR2023/papers/Sadasivan_CUDA_Convolution-Based_Unlearnable_Datasets_CVPR_2023_paper.pdf)
|2024.02.16|Marilyn Lionts     <br>  （Unlearnable Datasets）  |[《Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples》(CVPR 2023）](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Unlearnable_Clusters_Towards_Label-Agnostic_Unlearnable_Examples_CVPR_2023_paper.pdf)
|2024.02.09|Quan Liu     <br>  （Multi-modal Large Language Models (MLLM）  |[《Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization》（ArXiv）](https://arxiv.org/pdf/2309.04669.pdf)
|2024.02.09|Quan Liu     <br>  （MLLM）  |[《GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest》(ArXiv）](https://jshilong.github.io/images/gpt4roi.pdf)
|2024.02.09|Quan Liu     <br>  （MLLM）  |[《DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding》(ArXiv）](https://arxiv.org/pdf/2311.11810.pdf)
|2024.02.02 | Ruining Deng  <br> （Hierarchical Semantic Segmentation）  |[《Deep Hierarchical Semantic Segmentation》 （CVPR2022)](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Deep_Hierarchical_Semantic_Segmentation_CVPR_2022_paper.pdf)
|2024.02.02 | Ruining Deng  <br> （Hierarchical Semantic Segmentation）  |[《Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers 》 （CVPR2022）](https://openaccess.thecvf.com/content/CVPR2022/papers/Ke_Unsupervised_Hierarchical_Semantic_Segmentation_With_Multiview_Cosegmentation_and_Clustering_Transformers_CVPR_2022_paper.pdf)
|2024.02.02 | Ruining Deng  <br> （Universal segmentation）  |[《UniverSeg: Universal Medical Imaging Segmentation》 （ICCV2023](https://arxiv.org/pdf/2304.06131.pdf)
|2024.01.26|Can(Cathy) Cui     <br>  （Vision Language Model）  |[《LISA: Reasoning Segmentation via Large Language Model》 （ArXiv）](https://arxiv.org/pdf/2308.00692.pdf)
|2024.01.26|Can(Cathy) Cui     <br>  （Vision Language Model）  |[《Making Large Multimodal Models Understand Arbitrary Visual Prompts 》(ArXiv）](https://arxiv.org/pdf/2312.00784.pdf)
|2024.01.26|Can(Cathy) Cui     <br>  （Network Structure）  |[《U-Mamba Enhancing Long-range Dependency for Biomedical Image Segmentation》(ArXiv）](https://wanglab.ai/u-mamba.html)
|2024.01.12 | Yucheng Tang  <br> （Efficient ViT）  |[《EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction》 （ICCV 2023）](https://arxiv.org/pdf/2205.14756.pdf)
|2024.01.12 | Yucheng Tang  <br> Sparse ViT）  |[《SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer》 (CVPR) 2023）](https://arxiv.org/pdf/2303.17605.pdf)
|2024.01.12 | Yucheng Tang  <br> （Open-Vocabulary SAM）  |[《Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively》](https://arxiv.org/pdf/2401.02955.pdf)
|2023.11.17 | Dr. Huo  <br> （Spatial Transcriptomics）  |[《Visualization and Analysis of Gene Expression in Tissue Sections by Spatial Transcriptomics》 （Science 2016）](https://www.science.org/doi/10.1126/science.aaf2403?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed)
|2023.11.17 | Dr. Huo  <br> （Spatial Transcriptomics）  |[《Spatially Resolved Transcriptomes—Next Generation Tools for Tissue Exploration》 （BioEssay 2020）](https://onlinelibrary.wiley.com/doi/full/10.1002/bies.201900221)
|2023.11.17 | Dr. Huo  <br> （Spatial Transcriptomics）  |[《Alignment and Integration of Spatial Transcriptomics Data》 （Nature Method 2022）](https://www.nature.com/articles/s41592-022-01459-6)
|2023.11.10 | Quan Liu  <br> （Vision Language Foundation Model）  |[《Multimodal Few-Shot Learning with Frozen Language Models》 （NeruIPS 2021）](https://arxiv.org/pdf/2106.13884.pdf)
|2023.11.10 | Quan Liu  <br> （Vision Language Foundation Model）  |[《Frozen Transformers in Language Models Are Effective Visual Encoder Layers》 （arxiv）](https://arxiv.org/pdf/2310.12973.pdf)
|2023.11.10 | Quan Liu  <br> （Tranformer CNN backbone comparison）  |[《ConvNets Match Vision Transformers at Scale》 （DeepMind）](https://arxiv.org/pdf/2310.16764.pdf)
|2023.11.03 | Junlin Guo  <br> （Long-Tailed Learning + Knowledge Distillation）  |[《Long-Tailed Visual Recognition via Self-Heterogeneous Integration with Knowledge Excavation》 （CVPR 2023）](https://arxiv.org/pdf/2304.01279.pdf)
|2023.11.03 | Junlin Guo  <br> （Universal instance cell segmentation）  |[《Cellpose: a generalist algorithm for cellular segmentation》 （Nature. 2021）](https://www.nature.com/articles/s41592-020-01018-x)
|2023.11.03 | Junlin Guo  <br> （Universal instance cell segmentation + Harmony）  |[《MEDIAR: Harmony of Data-Centric and Model-Centric for Multi-Modality Microscopy》 （NeurIPS 2022）](http://arxiv.org/abs/2212.03465)
|2023.10.27 | Marilyn Lionts  <br> （Variational Autoencoders and Active Learning）  |[《An Active Learning Method Based on Variational Autoencoder and DBSCAN Clustering》 （2021）](https://downloads.hindawi.com/journals/cin/2021/9952596.pdf)
|2023.10.27 | Marilyn Lionts  <br> （Variational Autoencoders and Active Learning）  |[《The Power of Ensembles for Active Learning in Image Classification》 （CVPR 2018）](https://openaccess.thecvf.com/content_cvpr_2018/papers/Beluch_The_Power_of_CVPR_2018_paper.pdf)
|2023.10.27 | Marilyn Lionts  <br> （Variational Autoencoders and Active Learning）  |[《Variational Adversarial Active Learning》 （ICCV 2019）](https://openaccess.thecvf.com/content_ICCV_2019/papers/Sinha_Variational_Adversarial_Active_Learning_ICCV_2019_paper.pdf)
|2023.10.20 | Can(Cathy) Cui  <br> （Anomaly Detection and Localization）  |[《Anomaly Detection via Reverse Distillation from One-Class Embedding》 （CVPR2022）](https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_Anomaly_Detection_via_Reverse_Distillation_From_One-Class_Embedding_CVPR_2022_paper.pdf)
|2023.10.20 | Can(Cathy) Cui  <br> （Anomaly Detection and Localization）  |[《Revisiting Reverse Distillation for Anomaly Detection》 （CVPR2023）](https://openaccess.thecvf.com/content/CVPR2023/papers/Tien_Revisiting_Reverse_Distillation_for_Anomaly_Detection_CVPR_2023_paper.pdf)
|2023.10.20 | Can(Cathy) Cui  <br> （Anomaly Detection and Localization）  |[《ReContrast: Domain-Specific Anomaly Detection via Contrastive Reconstruction》 （NeurIPS）](https://arxiv.org/abs/2306.02602)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model）  |[《Flamingo: a Visual Language Model for Few-Shot Learning》 (DeepMind)](https://arxiv.org/abs/2204.14198)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model）  |[《PaLM: Scaling Language Modeling with Pathways》 (Google)](https://arxiv.org/abs/2204.02311)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model）  |[《PaLM-E: An Embodied Multimodal Language Model》 (Google)](https://arxiv.org/abs/2303.03378)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model）  |[《GPT-4 Technical Report 》 (OPEN AI)](https://arxiv.org/abs/2303.08774)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model）  |[《LLaMA: Open and Efficient Foundation Language Models》 (Meta)](https://arxiv.org/abs/2302.13971)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model）  |[《LLAVA: Visual Instruction Tuning》 (Microsoft, UWM)](https://arxiv.org/abs/2304.08485)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model --- Medical）  |[《Med-PALM : Large Language Models Encode Clinical Knowledge》 (Google)](https://arxiv.org/abs/2212.13138)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model --- Medical）  |[《BioMedCLIP: LARGE-SCALE DOMAIN-SPECIFIC PRETRAINING FOR BIOMEDICAL VISION-LANGUAGE PROCESSING》 (Microsoft)](https://arxiv.org/pdf/2303.00915.pdf)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model --- Medical）  |[《LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day 》 (Microsoft)](https://arxiv.org/pdf/2306.00890.pdf)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model --- Medical）  |[《Med-Flamingo: MED-FLAMINGO: A MULTIMODAL MEDICAL FEWSHOT LEARNER 》 (Stanford)](https://arxiv.org/pdf/2307.15189.pdf)
|2023.10.13 | Yucheng Tang  <br> （Vision Language Foundation Model --- Medical）  |[《Towards Generalist Foundation Model for Radiology 》 (Shanghai AI Lab)](https://arxiv.org/pdf/2308.02463.pdf)
|2023.10.6 | Dr. Huo  <br> （Vision language model）  |[《CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection》 （arxiv）](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.pdf)
|2023.10.6 | Dr. Huo  <br> （Fast data curation）  |[《Annotating 8,000 Abdominal CT Volumes for Multi-Organ Segmentation in Three Weeks》 （ICCV 2023）](https://arxiv.org/pdf/2305.09666.pdf)
|2023.10.6 | Dr. Huo  <br> （Tranformer backbone）  |[《UNesT: Local Spatial Representation Learning with Hierarchical Transformer for Efficient Medical Segmentation》 （MeDIA 2023）](https://browse.arxiv.org/pdf/2209.14378.pdf)
|2023.9.22 | Tianyuan Yao  <br> （Vision language model）  |[《BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning》 （AAAI 2023）](https://arxiv.org/pdf/2206.08657.pdf)
|2023.9.22 | Tianyuan Yao  <br> （Vision language model）  |[《PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents》 （MICCAI 2023）](https://arxiv.org/pdf/2303.07240.pdf)
|2023.9.22 | Tianyuan Yao  <br> （Representation disentanglement + segmentation）  |[《Directional Connectivity-based Segmentation of Medical Images》 （CVPR 2023）](https://arxiv.org/ftp/arxiv/papers/2304/2304.00145.pdf)
|2023.9.22 | Tianyuan Yao  <br> （Semi-supervised Segmentation）  |[《Orthogonal Annotation Benefits Barely-supervised Medical Image Segmentation》 （CVPR 2023）](https://arxiv.org/pdf/2303.13090.pdf)
|2023.9.15 | Ruining Deng  <br> （Prompt-based Segmentation）  |[《Incrementer: Transformer for Class-Incremental Semantic Segmentation with Knowledge Distillation Focusing on Old Class》 （CVPR2023）](https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Incrementer_Transformer_for_Class-Incremental_Semantic_Segmentation_With_Knowledge_Distillation_Focusing_CVPR_2023_paper.pdf)
|2023.9.15 | Ruining Deng  <br> （Prompt-based Segmentation）  |[《SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning》 （ICCV）](https://arxiv.org/pdf/2308.06531.pdf)
|2023.9.15 | Ruining Deng  <br> （Prompt-based Segmentation）  |[《ProSFDA: Prompt Learning based Source-free Domain Adaptation for Medical Image Segmentation》 （ArXiv）](https://arxiv.org/abs/2211.11514)
|2023.9.08|Dr. Huo     <br>  （Text-to-image Segmentation）  |[《Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models》 （ArXiv）](https://arxiv.org/pdf/2303.04803.pdf)
|2023.9.08|Dr. Huo     <br>  （Fundation Models）  |[《DINOv2 from Meta AI – Finally a Foundational Model in Computer Vision》 （Web Site）](https://aipapersacademy.com/dinov2-from-meta-ai-finally-a-foundational-model-in-computer-vision/) [(ArXiv)](https://arxiv.org/abs/2304.07193)
|2023.9.08|Dr. Huo     <br>  （Fundation Models）  |[《SAM-Med2D》 （ArXiv）](https://arxiv.org/pdf/2308.16184.pdf)
|2023.8.25|Quan Liu     <br>  （Self-supervised Learning）  |[《EMP-SSL: Towards Self-Supervised Learning in One Training Epoch》 （CVPR 2023）](https://arxiv.org/abs/2304.03977)
|2023.8.25|Quan Liu     <br>  （Vision language model + zero-shot learning）  |[《Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images》 （CVPR 2023）](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.pdf)
|2023.8.25|Quan Liu     <br>  （Image perturbation）  |[《Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation》 （CVPR 2023）](https://arxiv.org/pdf/2208.09910.pdf)





## Pool of great papers from the team (Senior folks can drop papers here as potential papers to review)
1. Ye, Shuquan, et al. "Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. [from Yuankai Huo]

1. Xie, Ronald, et al. "MAESTER: Masked Autoencoder Guided Segmentation at Pixel Resolution for Accurate, Self-Supervised Subcellular Structure Recognition." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. [from Yuankai Huo]

1. Huang, Zhi, et al. "A visual–language foundation model for pathology image analysis using medical Twitter." Nature Medicine (2023): 1-10. [from Yuankai Huo]
